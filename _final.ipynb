{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from runners import Result\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_curve, f1_score, auc, roc_auc_score, matthews_corrcoef, precision_score, recall_score\n",
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \"\"\"\n",
    "    Metrics for a given algo/dataset combo\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def average(metric_name, results):\n",
    "        return np.average([r.cv_results_[f'mean_test_{metric_name}'][r.best_index_] for r in results])\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_auc(results):\n",
    "        return np.average([r.test_AUC for r in results])\n",
    "\n",
    "    def __init__(self, results: list[Result]):\n",
    "        self.average_accuracy = Metrics.average('Accuracy', results)\n",
    "        self.average_f1_score = Metrics.average('F1', results)\n",
    "        self.average_auc = Metrics.average('AUC', results)\n",
    "        self.average_mcc = Metrics.average('MCC', results)\n",
    "        self.average_precision = Metrics.average('Precision', results)\n",
    "        self.average_recall = Metrics.average('Recall', results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import classifiers and their respective datasets \n",
    "runs = os.listdir('classifier_cache')\n",
    "# Get most recent run\n",
    "runs.sort(reverse=True, key=lambda k: int(k))\n",
    "chosen_run = runs[0]\n",
    "run_dir = f'classifier_cache/{chosen_run}'\n",
    "\n",
    "results_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for algo_name in os.listdir(run_dir):\n",
    "    algo_dir = f'{run_dir}/{algo_name}'\n",
    "    for dataset_name in os.listdir(algo_dir):\n",
    "        dataset_dir = f'{algo_dir}/{dataset_name}'\n",
    "        for run_name in os.listdir(dataset_dir):\n",
    "            run_file_name = f'{dataset_dir}/{run_name}'\n",
    "            # Read in the file\n",
    "            result = joblib.load(run_file_name)\n",
    "            results_dict[algo_name][dataset_name].append(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = {}\n",
    "for algo in results_dict.keys():\n",
    "    for dataset in results_dict[algo].keys():\n",
    "        results = results_dict[algo][dataset]\n",
    "        metric_dict[f'{algo} \\\\ {dataset}'] = vars(Metrics(results))\n",
    "\n",
    "metric_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "metric_df.to_latex('table_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}