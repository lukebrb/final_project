{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from runners import Result\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_curve, f1_score, auc, roc_auc_score, matthews_corrcoef, precision_score, recall_score\n",
    "from functools import reduce\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    \"\"\"\n",
    "    Metrics for a given algo/dataset combo\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def average(metric_name, results):\n",
    "        return np.average([r.cv_results_[f'mean_test_{metric_name}'][r.best_index_] for r in results])\n",
    "    \n",
    "    @staticmethod\n",
    "    def average_auc(results):\n",
    "        return np.average([r.test_AUC for r in results])\n",
    "\n",
    "    def __init__(self, results: list[Result]):\n",
    "        self.average_accuracy = Metrics.average('Accuracy', results)\n",
    "        self.average_f1_score = Metrics.average('F1', results)\n",
    "        self.average_auc = Metrics.average('AUC', results)\n",
    "        self.average_mcc = Metrics.average('MCC', results)\n",
    "        self.average_precision = Metrics.average('Precision', results)\n",
    "        self.average_recall = Metrics.average('Recall', results)\n",
    "\n",
    "    def average_all_metrics(self):\n",
    "        return np.average([self.average_accuracy, self.average_f1_score, self.average_auc, self.average_mcc, self.average_precision, self.average_recall])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import classifiers and their respective datasets \n",
    "runs = os.listdir('classifier_cache')\n",
    "# Get most recent run\n",
    "runs.sort(reverse=True, key=lambda k: int(k))\n",
    "chosen_run = runs[0]\n",
    "run_dir = f'classifier_cache/{chosen_run}'\n",
    "\n",
    "results_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "for algo_name in os.listdir(run_dir):\n",
    "    algo_dir = f'{run_dir}/{algo_name}'\n",
    "    for dataset_name in os.listdir(algo_dir):\n",
    "        dataset_dir = f'{algo_dir}/{dataset_name}'\n",
    "        for run_name in os.listdir(dataset_dir):\n",
    "            run_file_name = f'{dataset_dir}/{run_name}'\n",
    "            # Read in the file\n",
    "            result = joblib.load(run_file_name)\n",
    "            results_dict[algo_name][dataset_name].append(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = {}\n",
    "for algo in results_dict.keys():\n",
    "    for dataset in results_dict[algo].keys():\n",
    "        results = results_dict[algo][dataset]\n",
    "        metric_dict[f'{algo} \\\\ {dataset}'] = vars(Metrics(results))\n",
    "\n",
    "metric_df = pd.DataFrame.from_dict(metric_dict, orient='index')\n",
    "metric_df.to_latex('table_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_3_dict = defaultdict(dict)\n",
    "metrics = [\"average_accuracy\",\"average_f1_score\",\"average_auc\",\"average_mcc\",\"average_precision\",\"average_recall\"]\n",
    "ds_count = 0\n",
    "for algo in results_dict.keys():\n",
    "    for dataset in results_dict[algo].keys():\n",
    "        ds_count += 1\n",
    "        table_3_dict[algo][dataset] = Metrics(results_dict[algo][dataset]).average_all_metrics()\n",
    "\n",
    "pd.DataFrame.from_dict(table_3_dict, orient='index').to_latex('Table3.txt', column_format='p{0.2\\linewidth}' + 'p{0.1\\linewidth}' * ds_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}